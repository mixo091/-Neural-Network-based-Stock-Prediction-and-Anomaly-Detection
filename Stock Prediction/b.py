# -*- coding: utf-8 -*-
"""B.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ro6kw4x550qD7tfzc_qQwaRQaQ0QAz0f
"""



# ----- number of Time Series to dectect Anomalies ----
n = 5 
mae = 0.15

# Commented out IPython magic to ensure Python compatibility.
#Imports..
import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from pandas.plotting import register_matplotlib_converters
# %matplotlib inline
# %config InlineBackend.figure_format='retina'
register_matplotlib_converters()
sns.set(style='whitegrid', palette='muted', font_scale=1.5)
rcParams['figure.figsize'] = 22, 10

"""#Step 1 : getting and saving the data appropriately.

"""

data = pd.read_csv("/content/nasdaq2007_17.csv",sep='\t',header=None)
print(data)

# --- Keep only the prices for now --- 
stock_prices = data.drop(columns=0)
# --- Get the stocks --- .
stocks =  []
for i in range(0 , len(stock_prices)):
  stocks.append(   pd.DataFrame( np.array(stock_prices.iloc[i]).reshape(1,-1).T  , columns= ["close" ] ) )
# --- Now stocks list keeps the stocks as (3650,1) arrays. 
number_of_stocks = len(stocks)
print(len(stocks))
print(stocks[1].shape)
print(stocks[1])

"""
#Step 2: Calculating Train and Test sets.
"""

# --- Calculating set sizes ---

train_size = int( len(stocks[1]) *  0.80)
test_size =  len(stocks[1])  - train_size

print("Train size :" + str(train_size))
print("Test size :" + str(test_size))

train_set = []
test_set = []

for i in range(0 , len(stocks)):
  train_set.append(stocks[i][0:train_size])
  test_set.append(stocks[i][train_size:])

print(len(train_set[1]))
print(train_set[1])
print(" ---------------------------------- ")
print(len(test_set[1]))

"""#Step 3 : Let's scale our Data."""

# --- Scale Data ---
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

scaled_train_set = []
scaler = scaler.fit(train_set[1])
for i in range(0,len(train_set)):
  scaled_train_set.append(scaler.transform(train_set[i]))

# -------------------------------------------------------------------------- # 
scaled_test_set = []
scaler = scaler.fit(test_set[1])
for i in range(0,len(test_set)):
  scaled_test_set.append(scaler.transform(test_set[i]))

print(scaled_train_set[1])
print(scaled_train_set[1].shape)
print(scaled_test_set[1].shape)



"""
# Step 4 : Concatanate the chossen set of training stocks."""

#--- Concatanate scaled train set ----
# i will use 50 stocks in this set.
con_train_set = np.concatenate([scaled_train_set[0],scaled_train_set[1]])
for i in range(2,50):
  con_train_set= np.concatenate([con_train_set,scaled_train_set[i]])

print(con_train_set.shape)

"""#Step 5 : Split data Into subSeuquences ."""

# a function to do that

def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X.iloc[i:(i + time_steps)].values
        Xs.append(v)
        ys.append(y.iloc[i + time_steps])
    return np.array(Xs), np.array(ys)

TIME_STEPS = 20


train = pd.DataFrame(con_train_set, columns = ['close'])

print(len(scaled_test_set))
print(number_of_stocks)

test = []
for i in range(0 , n ):
  test.append (pd.DataFrame(scaled_test_set[number_of_stocks -1 - i], columns=['close']))

X_train, y_train = create_dataset(
  train[['close']],
  train.close,
  TIME_STEPS
)

X_test_list = []
y_test_list = []

for i in range(0 , n ):
  X_test, y_test = create_dataset(test[i][['close']], test[i].close, TIME_STEPS)
  X_test_list.append(X_test)
  y_test_list.append(y_test)

print(X_train.shape)
print(X_test_list[1].shape)
print(X_test_list[2].shape)

"""#Step 6 : Let's create our Autoencoder in *Keras* and Train our Model -- Experimenting Stage --
Our Autoencoder should take a sequence as input and outputs a sequence of the same shape. Hereâ€™s how to build such a simple model in Keras:

model = keras.Sequential()
model.add(keras.layers.LSTM(
    units=64,
    input_shape=(X_train.shape[1], X_train.shape[2])
))
model.add(keras.layers.Dropout(rate=0.5))
model.add(keras.layers.RepeatVector(n=X_train.shape[1]))
model.add(keras.layers.Dropout(rate=0.5))
model.add(keras.layers.LSTM(units=64, return_sequences=True))
model.add(keras.layers.Dropout(rate=0.5))
model.add(
  keras.layers.TimeDistributed(
    keras.layers.Dense(units=X_train.shape[2])
  )
)
model.compile(loss='mae', optimizer='adam')

"history = model.fit(
    X_train, y_train,
    epochs=8,
    batch_size=248,
    validation_split=0.4,
    shuffle=False
 )""
"""

import os.path
if os.path.isfile("./finalModel.h5") is False:
  model.save("./finalModel.h5")

from tensorflow.keras.models import load_model
new_model = load_model("./finalModel.h5")
new_model.summary()
model = new_model

# plt.plot(history.history['loss'], label='train')
# plt.plot(history.history['val_loss'], label='test')
# plt.legend();

X_train_pred = model.predict(X_train)


train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)

"""# Step 7 : Finding Anomalies.

"""

THRESHOLD = mae


test_score_df = []
dfs = []
for i in range(0,n):
  plt.figure()
  df = pd.DataFrame(scaled_test_set[i], columns = ['close'])
  dfs.append(df)
  #print(df)
  X_test_pred = model.predict(X_test_list[i])
  test_mae_loss = np.mean(np.abs(X_test_pred - X_test_list[i]), axis=1)
  test_score_df.append(pd.DataFrame(index =df[TIME_STEPS:].index))
  test_score_df[i]['loss'] = test_mae_loss
  test_score_df[i]['threshold'] = THRESHOLD
  test_score_df[i]['anomaly'] = test_score_df[i].loss > test_score_df[i].threshold
  test_score_df[i]['close'] = df[TIME_STEPS:].close
  plt.plot(test_score_df[i].index, test_score_df[i].loss, label='loss')
  plt.plot(test_score_df[i].index, test_score_df[i].threshold, label='threshold')
  plt.xticks(rotation=25)
  plt.legend();

anomalies = []
  for i in range(0,n):
    anomalies.append ( test_score_df[i][test_score_df[i].anomaly == True] )
    print(anomalies[i].head())

for i in range(0,n):
    plt.figure()
    if(len(anomalies[i]) >= 1):
  
        new = np.array(dfs[i][TIME_STEPS:].close).reshape(1,-1).T
        print(new.shape)
        anomalies_new = np.array(anomalies[i].close).reshape(1,-1).T
        print(anomalies_new.shape)
        scaled = scaler.inverse_transform(anomalies_new)
        d = scaled.flatten()
        print(d.shape)

        plt.plot(
          dfs[i][TIME_STEPS:].index, 
          scaler.inverse_transform(new ), 
          label='close price'
        );

        sns.scatterplot(
          anomalies[i].index,
          d,
          color=sns.color_palette()[3],
          s=52,
        
        )
        plt.xticks(rotation=25)
        plt.legend();